{\rtf1\ansi\ansicpg1252\cocoartf1138\cocoasubrtf510
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid1\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 make a data_genrf folder with subdirectories for experiments, datasets, etc\
import ICA code over\
import vision dataset over\
make it easy to launch a big server job--possibly modify blam to make it more user friendly--you just give it a command line to run, not an executable ideally.\
What would this look like, best case? you provide a list of parameters, and it goes and runs it? communicate through files? how it is now, you communicate through a matlab .mat file with a special struct that has all params for the experiments. sort of nice but sort of overkill.\
train some vision datasets\
get the analysis incorporated\
get a basic ABC posterior analysis going\
\
\
\
Can I reuse the data I have already simulated? \
would like a big set of matrices\
X has the inputs [weight decay, etc]\
Y has the outputs [L1 distances, etc]\
try it first with the ica data\
\
seemed to work pretty well--already yielded some interesting tradeoffs\
observation, though, that even 600 samples gave a very mediocre rendition of the true posterior. This to me implies that we would do well to try ABCDE. I think I'll try this, focusing on K-means. Need to write a good matlab implementation of ABCDE. We'd get something more like 60,000 samples once the cluster is back up.\
\
todo:\
import vision ICA code over--or run it in place from where it's sitting in pllocal? I guess I think it might be good to refactor a bit\'85 somewhat sad. I'm not such a great architect am I. \
\
Design philosophy: keep everything as simple matrices, not as named things in structs. have a simple matrix and a paired cell array of parameter names\
keep things function oriented and flat\
\
save whitening matrices for data from size 4x4 to 40x40, say. save only V and var. do it for 50k random patches. I assume this is faster than recomputing it. done\
\
import k-means code over done\
write a load_vision_data fn done\
k-means params: num_bases, winsz, pcs, rand seed done\
maybe use 10k training examples to start? done\
how to do priors that limit certain parameters? e.g., p(wins,pcs) is zero for pcs > winsz. this is fine. cool. I guess the question is should the prior be uniform over the lower triangle, or should it be independent? i guess independent seems more natural? but maybe not. one way, smaller pcs are preferred in the prior. The other way, larger win sizes are preferred. I think larger win sizes should not be advantaged.\
import vision data fitting code\
--may as well be careful about it\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural
\ls1\ilvl0\cf0 {\listtext	1.	}make sure it saves the associated scale factor (will be useful for rachel)\
{\listtext	2.	}make sure that it really is a scale factor, and that we're doing this comparison accurately\
{\listtext	3.	}think about how to refactor the code just to double check\
{\listtext	4.	}Come back to this...\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 \
Maybe to start get the whole end to end system running with just van hateren\
Do I need to \
\
goals for the moment:\
write fns to plug ABCDE into cluster, get it running\
review one NIPS2013 paper\
respond to dayan/barry/raheel\
\
\
\
\
}